# The Google File System Sanjay Ghemawat, Howard Gobioff, and Shun-Tak Leung SOSP 2003

map/reduce 使用的文件系统，为了后续设计的性能，放弃了原本系统文件良好的一致性．从应用程序的所有细节到网络性能，容错，一致性影响很大其他使用 GFS（例如，bigtable）基于 GFS 的 HDFS（Hadoop 的分布式文件系统）的系统．

首先，什么是一致性？数据保持一个正确的状态在备份或者是同时被多个应用访问的时候是非常重要的，考虑几个问题，当出现一个 later read 进行 observe 时，会怎么样？如果读取来自不同的应用程序呢？弱一致性 read（）可能返回陈旧的数据，不是最近写入的结果．强一致性 read（）总是返回最近 write（）的数据．一般的权衡策略是，强一致性对于 application writer 来说是很好的，但对于性能来说则影响很大．之后，会详细讨论这个问题．

一个理想状态下的一致性模型，一个备份过的文件应该表现像单个文件一样：许多客户机在同一台机器上访问单个磁盘上的文件如果一个应用程序写入，后来的读取会观察到写什么，如果两个应用程序同时写入同一个文件在文件系统中－＞文件可能有一些混合的内容．如果两个应用程序同时写入同一个目录，一个是第一个，另一个是第二个会怎么样呢？

*Challenges to achieving ideal consistency Concurrency Machine failures Network partitions*

为什么难以克服这些挑战：客户端和服务器之间的通信的要求，可能会降低性能协议，进而可能变得复杂（好难翻译～．～，Requires communication between clients and servers May cost performance Protocols can become complex）

GFS 的中心挑战：有这么多的机器故障是常见的假设一台机器每年失败一次 W / 1000 台机器，〜3 将失败每一天。高性能：许多并发的读写器 Map / Reduce 作业读取并将最终结果存储在 GFS Note 中．不是临时的，中间文件有效地使用网络，这些难题与 “理想” 的一致性很难结合．

问：为什么 3x 复制？除了数据的可用性，3x 复制给我们什么？读取到热文件关联的负载平衡

问：为什么不把每个文件的一个副本存储在 RAID 磁盘上？ RAID 不是想要整机的容错性; 不只是存储设备问：为什么大块这么大？

为什么大块这么大？ GFS 主服务器知道 dir 的目录层次结构，文件中包含哪些文件，知道每个 64 MB 主服务器的块服务器在内存中保持状态每个块拥有 64 个字节的元数据主服务器具有用于元数据的专用可恢复数据库主服务器可以从电源故障中快速恢复影子 master, 稍微落后一点 master，可以晋升为 master.

基本的文件操作: 客户端读取：发送文件名称和偏移到 master,master 答复与具有该组块的服务器响应．

GFS 能达到 “理想” 的一致性吗？两种情况：目录和文件目录：是，但是... 是：强一致性（只有一个副本）但是：主机可能关闭，GFS 不可用影子主机可以为只读操作提供服务，这可能会返回过时的数据问：为什么不写操作？split-brain syndrome（见下一讲）文件：并不总是带有原子追加的突变如果主文件无法联系副本，则文件可能有重复的条目和漏洞，主要报告客户端客户端重试错误，主要选择新的偏移量记录在两个偏移量处被复制，而其他副本可能在一个偏移量处有一个洞 “不幸” 的客户机可以在短时间内读取陈旧的数据失败的突变使得块不一致主要块服务器更新块但是然后失败并且复制品已经失去日期客户端可能读取不是最新的块当客户端刷新租用时，它将了解新版本的＃突变，而没有原子追加数个客户端的数据，如果在非复制的 Unix 上混合并发写入，也会导致奇怪的结果你是使用原子追加或临时文件和原子重命名

作者声称弱一致性不是应用程序的大问题 大多数文件更新是仅附加更新应用程序可以在附加记录中使用 UID 来检测重复应用程序可能只读取较少的数据（但不是过时的数据）应用程序可以使用临时文件和原子重命名

性能（图 3）巨大的聚合吞吐量用于读取（3 份，分条）聚合 125 MB / 秒接近于饱和网络写入不同文件低于可能的最大作者责备他们的网络堆栈，导致延迟从一个副本传播到下一个块并发追加到由存储上一个块的服务器限制的单个文件

对于 MapReduce 应用程序的性能，容错性和一致性的总结案例研究在 GFS 中运行良好吗？巨大的顺序读取和写入追加巨大的吞吐量（3 份，分条）数据容错（3 份）在 GFS 中不太好？主小文件容错（主瓶颈）客户端可能会看到过时的数据附加可能重复

［GFS: Evolution on Fast-forward］（https://queue.acm.org/detail.cfm?id=1594206）

［blog]（http://highscalability.com/blog/2010/9/11/googles-colossus-makes-search-real-time-by-dumping-mapreduce.html）
